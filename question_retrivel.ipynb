{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/3_k_czmn2552n9rftmys1pwh0000gn/T/ipykernel_43136/3085968131.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  Data = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False)\n",
      "b'Skipping line 83032: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 154657: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 323916: expected 6 fields, saw 7\\n'\n",
      "/var/folders/tr/3_k_czmn2552n9rftmys1pwh0000gn/T/ipykernel_43136/3085968131.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Data = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# read data and preprocessings\n",
    "Data = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False)\n",
    "Data = Data.dropna()\n",
    "# drop the duplicate rows in question2\n",
    "Data = Data.drop_duplicates(subset=['question2'])\n",
    "\n",
    "# fillter the first 100 'question1' with 'is_duplicate' == 1.0\n",
    "Q1 = Data[Data['is_duplicate'] == 1.0].head(100)\n",
    "# keep only the 'question1' column\n",
    "Q1 = Q1['question1']\n",
    "\n",
    "Q2= Data['question2']\n",
    "# test Q2 as the first 1000 'question2'\n",
    "#Q2 = Data['question2'].head(1000)\n",
    "Q2 = Q2.astype(str) # make sure the type is string\n",
    "\n",
    "#Process the review column line by line to do text preprocessing\n",
    "def process_review(review):\n",
    "    # remove the punctuations and numbers\n",
    "    #review = re.sub(r'[^A-Za-z]+', ' ', review)\n",
    "    review = re.sub(r\"[^\\w\\s]+\", \"\", review)\n",
    "    # convert the review to lower case\n",
    "    review = review.lower()\n",
    "    # remove the stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # tokenize the words\n",
    "    word_tokens = word_tokenize(review)\n",
    "    filtered_review = [w for w in word_tokens if not w in stop_words]\n",
    "    # lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(w) for w in filtered_review]\n",
    "    # return the processed review\n",
    "    return lemmatized_review\n",
    "\n",
    "# process the train and test reviews\n",
    "Q1 = Q1.apply(process_review)\n",
    "Q2 = Q2.apply(process_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 272959 entries, 0 to 363190\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            272959 non-null  object \n",
      " 1   qid1          272959 non-null  object \n",
      " 2   qid2          272959 non-null  float64\n",
      " 3   question1     272959 non-null  object \n",
      " 4   question2     272959 non-null  object \n",
      " 5   is_duplicate  272959 non-null  float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 14.6+ MB\n"
     ]
    }
   ],
   "source": [
    "Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66327"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vocabulary\n",
    "vocabulary = set()\n",
    "for q in Q2:\n",
    "    for w in q:\n",
    "        vocabulary.add(w)\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = {}\n",
    "DF = {}\n",
    "# count the TF and DF\n",
    "for word in vocabulary:\n",
    "        DF[word] = 0\n",
    "        for index_q, q in enumerate(Q2):\n",
    "            if word in q:\n",
    "                DF[word] += 1\n",
    "                if word not in TF:\n",
    "                    TF[word] = [(index_q, q.count(word))]\n",
    "                else:\n",
    "                    TF[word].append((index_q, q.count(word)))\n",
    "\n",
    "# iterate the TF and convert the count into log scale\n",
    "for word in TF:\n",
    "    for i in range(len(TF[word])):\n",
    "        TF[word][i] = (TF[word][i][0], np.log10(1 + TF[word][i][1]))\n",
    "\n",
    "#convert DF into IDF\n",
    "N = len(Q2)\n",
    "for word in DF:\n",
    "    DF[word] = N / DF[word]\n",
    "\n",
    "# convert TF, IDF into TF-IDF\n",
    "for word in TF:\n",
    "    for i in range(len(TF[word])):\n",
    "        TF[word][i] = (TF[word][i][0], TF[word][i][1] * DF[word])\n",
    "\n",
    "# TF is already the inverted file\n",
    "inverted_file = TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 5 similar questions\n",
    "def get_top5_similar_questions(query):\n",
    "    ranking = {}\n",
    "    for word in query:\n",
    "        if word in inverted_file:\n",
    "            for index_q, score in inverted_file[word]:\n",
    "                if index_q not in ranking:\n",
    "                    ranking[index_q] = score\n",
    "                else:\n",
    "                    ranking[index_q] += score\n",
    "    ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranking[:2], ranking[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top2 accuracy:  0.23\n",
      "top5 accuracy:  0.38\n"
     ]
    }
   ],
   "source": [
    "Q1_dict = Q1.to_dict()\n",
    "acc_2 = 0\n",
    "acc_5 = 0\n",
    "for q in Q1_dict:\n",
    "    real_index = q\n",
    "    test_q = Q1_dict[q] \n",
    "    top2, top5 = get_top5_similar_questions(test_q)\n",
    "    if real_index in [i[0] for i in top2]:\n",
    "        acc_2 += 1\n",
    "    if real_index in [i[0] for i in top5]:\n",
    "        acc_5 += 1\n",
    "    \n",
    "print('top2 accuracy: ', acc_2/100)\n",
    "print('top5 accuracy: ', acc_5/100)\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence embedding(averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained glove word embeddings\n",
    "embeddings_dict = {}\n",
    "with open(\"glove/glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the value of Q1_dict into sentence embedding\n",
    "Q1_dict_vec = {}\n",
    "for q_index in Q1_dict:\n",
    "    q_vec = np.zeros(50)\n",
    "    for word in Q1_dict[q_index]:\n",
    "        if word in embeddings_dict:\n",
    "            q_vec += embeddings_dict[word]/len(Q1_dict[q])\n",
    "    Q1_dict_vec[q_index] = q_vec\n",
    "\n",
    "\n",
    "#convert the value of Q2_dict into sentence embedding\n",
    "Q2_dict = Q2.to_dict()\n",
    "Q2_dict_vec = {}\n",
    "for q_index in Q2_dict:\n",
    "    q_vec = np.zeros(50)\n",
    "    for word in Q2_dict[q_index]:\n",
    "        if word in embeddings_dict:\n",
    "            q_vec += embeddings_dict[word]/len(Q2_dict[q])\n",
    "    Q2_dict_vec[q_index] = q_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between 2 sentence embeddings\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 similar questions\n",
    "def get_top5_similar_questions2(query):\n",
    "    ranking = {}\n",
    "    for q in Q2_dict:\n",
    "        ranking[q] = cosine_similarity(query, Q2_dict_vec[q])\n",
    "    ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranking[:2], ranking[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/3_k_czmn2552n9rftmys1pwh0000gn/T/ipykernel_43136/2511995998.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / (norm_a * norm_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top2 accuracy:  0.59\n",
      "top5 accuracy:  0.66\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy\n",
    "acc_2 = 0\n",
    "acc_5 = 0\n",
    "for q in Q1_dict_vec:\n",
    "    real_index = q\n",
    "    test_q = Q1_dict_vec[q] \n",
    "    top2, top5 = get_top5_similar_questions2(test_q)\n",
    "    if real_index in [i[0] for i in top2]:\n",
    "        acc_2 += 1\n",
    "    if real_index in [i[0] for i in top5]:\n",
    "        acc_5 += 1\n",
    "\n",
    "print('top2 accuracy: ', acc_2/100)\n",
    "print('top5 accuracy: ', acc_5/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence embedding(based on thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for Q in Q2:\n",
    "    for word in Q:\n",
    "        corpus.append(word)\n",
    "\n",
    "# calculate the unigram probability of a word in the corpus\n",
    "def calculate_unigram_probability(word):\n",
    "    word_count = corpus.count(word)\n",
    "    total_words = len(corpus)\n",
    "    unigram_probability = word_count / total_words\n",
    "    return unigram_probability\n",
    "\n",
    "# create a dictionary to store the unigram probability of each word\n",
    "unigram_probabilities = {}\n",
    "for word in vocabulary:\n",
    "    unigram_probabilities[word] = calculate_unigram_probability(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66327"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_dict = Q1.to_dict() # in case, might need to run again\n",
    "Q2_dict = Q2.to_dict()\n",
    "\n",
    "def sentence_embedding(word_embeddings, sentences, a, word_probabilities):\n",
    "    sentence_embeddings = {}\n",
    "    for index, s in sentences.items():\n",
    "        vs = np.zeros(50)  # Initialize sentence embedding as zero vector\n",
    "        for w in s:\n",
    "            try:\n",
    "                a_value = a / (a + word_probabilities[w])  # Smooth inverse frequency, SIF\n",
    "                vs += a_value * word_embeddings[w] * (1/len(s)) # vs += sif * word_vector\n",
    "                #vs += ((word_embeddings[w] * a)/(a + word_probabilities[w]))* (1/len(s))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        sentence_embeddings[index] = vs\n",
    "\n",
    "    sentence_list = list(sentence_embeddings.values())\n",
    "    num_sentences = len(sentence_list)\n",
    "    embedding_dim = sentence_list[0].shape[0]  # Assuming all embeddings have the same dimension\n",
    "    X = np.zeros((embedding_dim, num_sentences))\n",
    "\n",
    "    for i, embedding in enumerate(sentence_list):\n",
    "        X[:, i] = embedding\n",
    "\n",
    "    # Perform singular value decomposition\n",
    "    u, _, _ = np.linalg.svd(X, full_matrices=False)  #full_matrices=False ensures that only the necessary number of singular vectors is returned\n",
    "    u = u[:, 0]  # Extract first singular vector\n",
    "\n",
    "    for index, s in sentences.items():\n",
    "        vs = sentence_embeddings[index]\n",
    "        uuT = np.outer(u, u)  # Compute the outer product of u with itself\n",
    "        vs = vs - np.dot(uuT, vs)  # Subtract the product of uuT and vs from vs\n",
    "        sentence_embeddings[index] = vs\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_dict_vec2 = sentence_embedding(embeddings_dict, Q1_dict, 0.5, unigram_probabilities)\n",
    "Q2_dict_vec2 = sentence_embedding(embeddings_dict, Q2_dict, 0.5, unigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/3_k_czmn2552n9rftmys1pwh0000gn/T/ipykernel_43136/2511995998.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / (norm_a * norm_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top2 accuracy:  0.61\n",
      "top5 accuracy:  0.71\n"
     ]
    }
   ],
   "source": [
    "# get the top 5 similar questions\n",
    "def get_top5_similar_questions3(query):\n",
    "    ranking = {}\n",
    "    for q in Q2_dict_vec2:\n",
    "        ranking[q] = cosine_similarity(query, Q2_dict_vec2[q])\n",
    "    ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranking[:2], ranking[:5]\n",
    "\n",
    "# get the accuracy\n",
    "acc_2 = 0\n",
    "acc_5 = 0\n",
    "for q in Q1_dict_vec2:\n",
    "    real_index = q\n",
    "    test_q = Q1_dict_vec2[q] \n",
    "    top2, top5 = get_top5_similar_questions3(test_q)\n",
    "    if real_index in [i[0] for i in top2]:\n",
    "        acc_2 += 1\n",
    "    if real_index in [i[0] for i in top5]:\n",
    "        acc_5 += 1\n",
    "\n",
    "print('top2 accuracy: ', acc_2/100)\n",
    "print('top5 accuracy: ', acc_5/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/3_k_czmn2552n9rftmys1pwh0000gn/T/ipykernel_43136/2511995998.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / (norm_a * norm_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  0.01\n",
      "top2 accuracy:  0.6\n",
      "top5 accuracy:  0.71\n",
      "a =  0.1\n",
      "top2 accuracy:  0.6\n",
      "top5 accuracy:  0.7\n",
      "a =  1\n",
      "top2 accuracy:  0.61\n",
      "top5 accuracy:  0.7\n",
      "a =  5\n",
      "top2 accuracy:  0.61\n",
      "top5 accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "# try different values of a\n",
    "for a in [0.01, 0.1, 1, 5]:\n",
    "    Q1_dict_vec2 = sentence_embedding(embeddings_dict, Q1_dict, a, unigram_probabilities)\n",
    "    Q2_dict_vec2 = sentence_embedding(embeddings_dict, Q2_dict, a, unigram_probabilities)\n",
    "    acc_2 = 0\n",
    "    acc_5 = 0\n",
    "    for q in Q1_dict_vec2:\n",
    "        real_index = q\n",
    "        test_q = Q1_dict_vec2[q] \n",
    "        top2, top5 = get_top5_similar_questions3(test_q)\n",
    "        if real_index in [i[0] for i in top2]:\n",
    "            acc_2 += 1\n",
    "        if real_index in [i[0] for i in top5]:\n",
    "            acc_5 += 1\n",
    "    print('a = ', a)\n",
    "    print('top2 accuracy: ', acc_2/100)\n",
    "    print('top5 accuracy: ', acc_5/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
